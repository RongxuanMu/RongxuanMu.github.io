<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Works and Contributions at PICO</title>
    <link rel="stylesheet" href="../project-styles.css">
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="icon" href="../Resources/Rongxuan.jpeg" type="image/x-icon">

</head>

<body>
    <header>
        <div class="image-container">
            <img src="../Resources/PICO.png" alt="PICO" style="width:100%;">
            <div class="overlay-text">Works at PICO</div>
        </div>
    </header>
    <!-- Navigation Bar -->
    <nav class="top-nav">
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#xri-framework">Interaction Framework</a></li>
            <li><a href="#os-cases">OS Cases</a></li>
            <li><a href="#XR+AI-HCI-research">XR+AI HCI Research</a></li>
            <li><a href="#gameplay">Gameplay & Prototypes</a></li>
            <li><a href="../index.html">‚Üê Home </a></li>


        </ul>
    </nav>


    <main>
        <!-- Home / Overview Section -->
        <section id="overview">
            <h2>Overview</h2>

            <p>
                üè¢As a leading XR company, <strong>PICO</strong> specializes in <strong>XR hardware and immersive
                    interaction technologies</strong>, driving innovation in spatial computing and user experience.
            </p>
            <p>
                üë®‚ÄçüíªDuring my time at <strong>PICO</strong>, I contributed to the <strong>definition and innovation of
                    XR
                    interaction frameworks</strong>, focusing on <strong>interaction standardization, novel input
                    methods, and immersive gameplay experience design</strong>. My work spanned <strong>technical
                    research, hardware-software exploration, and system architecture development</strong>, helping to
                enhance <strong>user interactions and interaction efficiency in XR environments</strong>.
            </p>
            <p style="color: darkblue;">
                üîíAll the content presented here has either been <strong>publicly released</strong>, is
                <strong>non-confidential</strong>, or has undergone <strong>proper de-identification</strong> to
                ensure compliance with information security and confidentiality standards.
            </p>

        </section>
        <!-- HCI Framework Section -->
        <section id="xri-framework">
            <h2>Interaction Framework</h2>


            <p>
                The <strong>XR Interaction Framework</strong> provides a structured approach to defining interaction
                paradigms across various XR platforms.
                It integrates <span class="highlight">interaction devices, event systems, solvers, and interactor
                    logic</span> to create seamless and intuitive
                user experiences in <strong>VR, AR, and MR</strong> environments.
            </p>
            <p>
                This framework references industry-standard toolkits:
            </p>
            <ul>
                <li> <strong>XRI</strong> (XR Interaction Toolkit) ‚Äì Unity's modular framework for handling
                    interactions.</li>
                <li> <strong>MRTK</strong> (Mixed Reality Toolkit) ‚Äì Optimized for HoloLens and cross-platform MR
                    development.</li>
                <li> <strong>AndroidXR</strong> ‚Äì Google's XR framework integrating hand-tracking, eye-tracking, and
                    controllers.</li>
                <li> <strong>VisionOS</strong> ‚Äì Apple's spatial computing framework for <em>visionOS
                        applications</em>.</li>
            </ul>
            <p>
                <strong>Since November 2023, I have been the owner of this project and a key contributor to the
                    prototypes.</strong></p>

            <h3>XR Interaction Framework Components</h3>
            <img src="../Resources/XRInteractionFramework.png" alt="HCI Framework">

            <h4>Interactive Devices</h4>
            <ul>
                <li><strong>Hardware Devices:</strong> Controllers, Mouse, Touchpad, Game Controllers, Wearables (Rings,
                    Wristbands).</li>

                <li><strong>Wetware Devices:</strong> Hands ‚úã, Eyes üëÄ, Head üßë‚Äçü¶±, Mouth üó£Ô∏è as biological input
                    interfaces.</li>
            </ul>

            <h4>Interactors</h4>
            <ul>
                <li>üëâ <strong>Direct Interactors:</strong> Pinch, Poke interactions etc.</li>

                <li>üéØ <strong>Indirect Interactors:</strong> Raycasting-based (Hand Ray, Controller Ray, Gaze Ray)Ôºå
                    etc.
                </li>
                <li>üé§ <strong>Functional Interactors:</strong> Teleport, Socket, Speech Input, etc.</li>
            </ul>
            <!--video clips-->

            <div style="display: flex; justify-content: center; gap: 20px;">
                <div style="text-align: center;">
                    <video width="auto" height="200" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/GazePinch.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>

                    <p style="font-style: italic; color: grey; font-size: smaller;">
                        General Gaze-pinch interaction with 3D objects.
                    </p>
                </div>
                <div style="text-align: center;">
                    <video width="200" height="200" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/GazePinch2DVisulized.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Visulization of gaze-pinch 2D
                        interactions.
                    </p>
                </div>


                <div style="text-align: center;">
                    <video width="200" height="200" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/GazePinch2D.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Detailed Optimization of gaze-pinch
                        gestures.
                    </p>
                </div>
            </div>


            <h4>Interaction Solvers</h4>
            <ul>
                <li>üîÑ <strong>Transforming:</strong> Position, Rotation, Scale.</li>

                <li>üîç <strong>Smoothing & Filtering:</strong> Enhancing precision.</li>
                <li>ü§ñ <strong>Dynamic Control:</strong> Adaptive interaction systems.</li>
            </ul>

            <h4>Interaction Events</h4>
            <ul>
                <li>üéà <strong>Primary Basic Events:</strong> HoverEntered, SelectEntered, SelectCanceled.</li>

                <li>üéõÔ∏è <strong>Secondary Complex Events:</strong> Tap/Click, Long Press, Drag.</li>
                <li>üé° <strong>Tertiary Events:</strong> Scale, Rotate, Gesture-based interactions.</li>
            </ul>

            <h3>Workflow of XR Interaction Process</h3>
            <br>
            <img src="../Resources/WorkFlowOfXRInteraction.png" alt="HCI Framework">


            <h4>1Ô∏è‚É£ User Inputs & Sensors</h4>
            <p>Interactive devices (hardware or wetware) collect user input via controllers, gestures, eye-tracking, or
                voice commands.</p>


            <h4>2Ô∏è‚É£ Interactor Processing</h4>
            <p>The system determines whether input is direct, indirect, or functional and triggers corresponding events
                (Raycasting, Gesture Tracking).</p>


            <h4>3Ô∏è‚É£ Interaction Solvers & Object Manipulation</h4>
            <p>Events are processed through solvers like <span class="highlight">Transforming, Smoothing</span>, and the
                system applies dynamic responses.</p>


            <h4>4Ô∏è‚É£ System Response & Feedback</h4>
            <ul>
                <li>üé® <strong>Visual Feedback:</strong> UI responses, highlights.</li>

                <li>üéÆ <strong>Haptic Feedback:</strong> Vibration, force feedback.</li>
                <li>üîä <strong>Auditory Feedback:</strong> Sound cues, voice prompts.</li>
            </ul>

            <h4>5Ô∏è‚É£ Final Execution & Loopback</h4>
            <p>The system updates interaction states and ensures real-time tracking for smooth user interaction.</p>


            <h3>Conclusion</h3>
            <p>
                This XR Interaction Framework and workflow provide a standardized, modular approach for designing

                intuitive
                and efficient user interactions in XR environments.
                By referencing <span class="highlight">XRI, MRTK, AndroidXR, and VisionOS</span>, this system ensures
                compatibility across multiple XR platforms.
                <br>
                <br>


                <strong>Since November 2023, I have been the owner of this project and a key contributor to the
                    prototypes.</strong>
            </p>
        </section>

        <!-- OS Cases Section -->
        <section id="os-cases">
            <h2>OS Cases</h2>
            <h3>Optimization of OS Interactions</h3>
            <p>
                In my role at <strong>PICO</strong>, I worked on optimizing operating system interactions to improve
                user engagement and system performance. This included refining <strong>gesture recognition
                    algorithms</strong> ü§è, enhancing <strong>multi-touch capabilities</strong> üëÜ, and ensuring smooth
                transitions between different interaction modes. Additionally, I developed a fuzzy-gaze-like strategy
                for eye-tracking üëÄ to enhance precision and responsiveness in user interactions.
            </p>
            <!--video clips-->
            <div style="display: flex; justify-content: center; gap: 40px;">
                <div style="text-align: center;">
                    <video width="380" height="380" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/DirectInteractionVideo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Prototype of optimized direct
                        interactions.
                    </p>
                </div>

                <div style="text-align: center;">
                    <video width="380" height="380" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/FuzzyStrategyDiagram.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Mechanism of the weighted-scoring
                        fuzzy strategy.
                    </p>
                </div>
            </div>
            <h3>Solutions for OS Apps & Services</h3>
            <p>
                I contributed to developing solutions for OS applications by implementing <strong>adaptive user
                    interfaces</strong> and integrating <strong>AI-driven interaction models</strong> ü§ñ. These
                solutions aimed to provide a personalized user experience, adapting to user preferences and behaviors
                across various <strong>XR platforms</strong>. Additionally, I worked on enhancing OS services such as
                the <strong>MR
                    safety boundary</strong> to ensure user safety, and developed <strong>spatial input methods</strong>
                to improve
                interaction precision and efficiency.
            </p>

            <!--video clips-->
            <div style="display: flex; justify-content: center; gap: 40px;">
                <div style="text-align: center;">
                    <video width="380" height="380" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/SaftyCollision.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Mechanism of the safety collision
                        detection.
                    </p>

                </div>

                <div style="text-align: center;">
                    <video width="380" height="380" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/SaftyArea.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Prototpe of embodied safty in MR
                        environments.
                    </p>
                </div>
            </div>
        </section>
        <!-- XR HCI Research Section -->
        <section id="XR+AI-HCI-research">
            <h2>XR+AI HCI Research</h2>
            <p>
                My research in <strong>XR+AI HCI</strong> focuses on integrating <strong>artificial
                    intelligence</strong> to enhance human-computer interaction in extended reality environments. This
                involves exploring innovative ways to leverage AI for improving user experience and interaction
                efficiency ü§ñ.
            </p>


            <h3>AI-Driven Interaction Paradigms</h3>
            <p>
                I explored the integration of AI with XR to create more intuitive and efficient interaction paradigms.
                This research focused on developing <strong>AI-driven systems</strong> that can adapt to user
                preferences and behaviors, providing a personalized and seamless user experience üåü.
            </p>

            <h3>Core Principles of XR UI/UX</h3>
            <p>
                The core of my research lies in understanding the fundamental principles of <strong>XR HCI</strong>,
                aiming to create intuitive and immersive user experiences. This includes studying user behavior,
                interaction patterns, and the impact of various XR technologies on user engagement üéÆ.
            </p>

            <h3>Enhancing XR Interaction Performance</h3>
            <p>
                I conducted extensive research on <strong>XR interaction performance</strong> to identify bottlenecks
                and optimize system responsiveness. This research helps in enhancing the fluidity and naturalness of
                interactions, ensuring a seamless user experience ‚ö°.
            </p>

            <h3>Innovations in XR Interaction Devices</h3>
            <p>
                My work on <strong>XR interaction devices</strong> involves evaluating and improving the effectiveness
                of various input devices, such as controllers, hand-tracking systems, and eye-tracking technologies üëÄ.
                This research aims to expand the possibilities of user interaction in XR environments üåü.
            </p>
            <!--video clips-->
            <div style="display: flex; justify-content: center; gap: 40px;">
                <div style="text-align: center;">
                    <video width="380" height="380" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/AugmentedSurfaceKeyboard.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>

                    <p style="font-style: italic; color: grey; font-size: smaller;">Augmented interaction with Touchpad.
                    </p>
                </div>

                <div style="text-align: center;">
                    <video width="380" height="380" autoplay muted loop style="border: 2px solid grey;">
                        <source src="../Resources/Ring.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-style: italic; color: grey; font-size: smaller;">Ring interaction prototype.
                       
                    </p>
                </div>
            </div>


        </section>
        <!-- Game Play Section -->
        <section id="gameplay">
            <h2>XR Gameplay & Prototypes</h2>

            <p>
                Before I came to the department of <strong>HCI</strong>, I worked as a <strong>technical
                    designer</strong> in the department of <strong>Tech and Art</strong>ÔºàMar 2022 - Jun 2023Ôºâ, working on some innovative
                gameplay for demonstrating the <strong>MR feature</strong> of the headset.
            </p>
            <p>
                My gameplay contributions lie in two specific projects:
            </p>
            <h3>MR Multi-Player Shooting Game</h3>
            <p>
                This project is a mixed reality multi-player shooting game with rich property features.
            </p>
            <div style="text-align: center;">
               
                    <video width="800" height="auto" autoplay muted loop >
                        <source src="../Resources/MRshooting.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>

                    <p style="font-style: italic; color: grey; font-size: smaller;">Recording of the MR shooting game.
                    </p>
               
            </div>

            <h3>2022 Chinese New Year Mixed-Reality Showcase</h3>
            <iframe
                src="https://player.bilibili.com/player.html?isOutside=true&aid=947638004&bvid=BV14W4y1V7zJ&cid=962281470&p=1&autoplay=0"
                width="800" height="450" scrolling="no" border="0" frameborder="no" framespacing="0"
                allowfullscreen="true"></iframe>
            <p style="text-align: center; font-style: italic; color: grey; font-size: smaller;">Bilibili video
                showcasing the 2022 Chinese New Year Mixed-Reality experience.</p>

            <p>
                This project demonstrates the potential of <strong>PICO 4</strong> in mixed reality (MR), featuring an
                operation activity during CNY 2022. It included a New Year gift box and several different props with MR
                features. I was in charge of the dragon dance wand that can trigger a spatial snake-like game.
            </p>
            <p>
                Watch the real-time effect recording of our outcome:
            </p>

            <!-- Videos Embed -->
            <div style="text-align: center;">
                <iframe width="800" height="450" src="https://www.youtube.com/embed/qhRl-X95-y4?si=BNjNf8wWtkO17T4W"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <p style="text-align: center; font-style: italic; color: grey; font-size: smaller;">YouTube video of the
                    real-time effect recording of the 2022 Chinese New Year showcase.</p>

            </div>

            <h3>Other Gameplay Prototypes</h3>
            <p>
                I also worked on some other gameplay prototypes, including <strong>interactive Boids</strong> and some <strong>spatial swags</strong>.
            </p>
            <div style="text-align: center;">
                <video width="800" height="auto" autoplay muted loop >
                    <source src="../Resources/SpatialSwags.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            
            


        </section>

       



        <!-- Contact Section with Social Media Links -->
        <section id="contact">

            <div class="social-media">
                <a href="https://www.linkedin.com/in/rongxuanmu/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="https://github.com/Rongxuan2024" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://space.bilibili.com/391939640" target="_blank"><i class="fab fa-bilibili"></i></a>
                <a href="https://www.youtube.com/@Xrchitect_Rong" target="_blank"><i class="fab fa-youtube"></i></a>
                <a href="https://murongxuan.itch.io/" target="_blank"><i class="fab fa-itch-io"></i></a>
                <a href="mailto:murongxuan1998@qq.com"><i class="fas fa-envelope"></i></a>
            </div>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Rongxuan. All rights reserved.</p>
    </footer>

</body>

</html>